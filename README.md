# Main source of info

https://gunenkov.notion.site/NLP-5-b115fc07b59a40e88ead62ec216e0ab3

# Practical questions

1) С помощью регулярного выражения удалить из текста сначала числа большие 1000 и меньшие 100000, а затем (другое выражение) все имена собственные и адреса электронной почты (использовать оператор ИЛИ). Выполнить предварительную обработку текстовых данных с использованием любых инструментов.

2) Обучить на данной преподавателем коллекции текстовых документов (3-5 предложений) Word2Vec модель с помощью Gensim. Сохранить модель на диск и затем загрузить ее.

3) С помощью PyTorch собрать (не обучать) модель с рекуррентными блоками LSTM для решения задачи классификации. (`./task_3__lstm.ipynb`)

4) С помощью PyTorch собрать (не обучать) Encoder для решения задачи машинного перевода. (`./task_4__encoder.ipynb`)

5) С помощью PyTorch собрать (не обучать) Decoder для решения задачи машинного перевода. (`./task_5__encoder.ipynb`) 

6) Используя torchdata загрузить из текстового файла небольшой набор данных для машинного перевода. Выполнить токенизацию, сформировать вокабуляр с помощью torchtext. 

7) Загрузить любую модель, предложенную преподавателем, с Hugging Face и решить определенную преподавателем задачу с помощью соответствующего пайплайна.

8) С помощью PyTorch собрать (не обучать) блок SelfAttention (на вход подавать Q, K, V). (`./task_8__self_attention_QKV.ipynb`)

9) Загрузить набор данных 20 newsgroups с помощью sklearn и провести тематическое моделирование с помощью BigARTM.

10) Загрузить набор данных 20 newsgroups с помощью sklearn и провести тематическое моделирование с помощью BertTopic.